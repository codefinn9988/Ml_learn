{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNve9LD3VFTaYGdxP8y6gPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codefinn9988/Ml_learn/blob/main/nn_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bIacY1VWPq-Z"
      },
      "outputs": [],
      "source": [
        "# Neural network from scratch using python and numpy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3-TBT4tMgTQ9"
      },
      "outputs": [],
      "source": [
        "class Layer_Dense:\n",
        "  def __init__(self, n_inputs, n_neurons,\n",
        "               weight_regularizer_l1 = 0,weight_regularizer_l2 = 0,\n",
        "               bias_regularizer_l1 = 0, bias_regularizer_l2 = 0):\n",
        "    self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons)) # 1 is shape\n",
        "\n",
        "    #Set regularization strength\n",
        "    self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "    self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "    self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "    self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  def backward(self,dvalues): #dvalues = gradient of loss with respect to the layer's output\n",
        "    #dl/dw (gradient of weights)\n",
        "    self.dweights =np.dot(self.inputs.T,dvalues)\n",
        "    #dl/db (gradient of bias)\n",
        "    self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
        "\n",
        "    # Gradients on regularization\n",
        "    # L1 on weights\n",
        "    if self.weight_regularizer_l1 >0:\n",
        "      dL1 = np.ones_like(self.weights)\n",
        "      dL1[self.weights<0] = -1\n",
        "      self.dweights +=self.weight_regularizer_l1 * dL1\n",
        "\n",
        "    #L2 on weights\n",
        "    if self.weight_regularizer_l2 > 0:\n",
        "      self.dweights += 2* self.weight_regularizer_l2 * self.weights\n",
        "\n",
        "\n",
        "    # L1 on biases\n",
        "    if self.bias_regularizer_l1 >0:\n",
        "      dL1 = np.ones_like(self.biases)\n",
        "      dL1[self.biases<0] = -1\n",
        "      self.dbiases +=self.bias_regularizer_l1 * dL1\n",
        "\n",
        "    #L2 on biases\n",
        "    if self.bias_regularizer_l2 > 0:\n",
        "      self.dbiases += 2* self.bias_regularizer_l2 * self.biases\n",
        "\n",
        "    #dl/dx (gradient of inputs)\n",
        "    self.dinputs = np.dot(dvalues,self.weights.T)\n",
        "\n",
        "\n",
        "#Dropout\n",
        "class Layer_Dropout:\n",
        "  def __init__(self,rate):\n",
        "    self.rate  = 1 - rate\n",
        "\n",
        "  #forward pass\n",
        "  def forward(self,inputs):\n",
        "    self.inputs = inputs\n",
        "    # Generate and save scaled mask\n",
        "    self.binary_mask = np.random.binomial(1,self.rate,size=inputs.shape)/self.rate\n",
        "    # Apply mask to output values\n",
        "    self.output= inputs * self.binary_mask\n",
        "\n",
        "  #backward pass\n",
        "  def backward(self,dvalues):\n",
        "    self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "# ReLU Function\n",
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "  def backward(self,dvalues):\n",
        "\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero gradient where input values were negative\n",
        "    self.dinputs[self.inputs<=0] = 0 #unchanged for >0\n",
        "\n",
        "# Softmax activaton function\n",
        "class Activation_Softmax:\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    #np.max use to handle overflow\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "    keepdims=True))\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "    keepdims=True)\n",
        "    self.output = probabilities\n",
        "\n",
        "  def backward(self,dvalues):\n",
        "    self.dinputs =np.empty_like(dvalues) #return a new array with same shape\n",
        "\n",
        "    for index, (single_output, single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
        "      single_output = single_output.reshape(-1,1)\n",
        "      #?\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output,single_output.T)\n",
        "\n",
        "      # Calculate sample-wise gradient\n",
        "      # and add it to the array of sample gradients\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues) # chain rule\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "  #forward pass\n",
        "  def forward(self,inputs):\n",
        "    # Save input and calculate/save output\n",
        "    # of the sigmoid function\n",
        "    self.inputs = inputs\n",
        "    self.output = 1/(1+ np.exp(-inputs))\n",
        "\n",
        "  #backward pass\n",
        "  def backward(self,dvalues):\n",
        "    #derivative\n",
        "    self.dinputs = dvalues * (1-self.output) * self.output\n",
        "\n",
        "# Linear activation\n",
        "class Activation_Linear:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "  # Just remember values\n",
        "    self.inputs = inputs\n",
        "    self.output = inputs\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "  # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
        "    self.dinputs = dvalues.copy()\n",
        "\n",
        "#optimizers\n",
        "\n",
        "#SGD Optimizer\n",
        "\n",
        "class Optimizer_SGD:\n",
        "  def __init__(self,learning_rate=1, decay=0,momentum= 0):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations =0\n",
        "    self.momentum= momentum\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1 / (1+ self.decay * self.iterations))\n",
        "\n",
        "\n",
        "  #update parameters\n",
        "  def update_params(self,layer):\n",
        "    # If layer does not contain momentum arrays, create them\n",
        "    # filled with zeros\n",
        "    if self.momentum:\n",
        "      if not hasattr(layer,'weight_momentums'): #hasattr = returns True if the specified object has the specified attribute, otherwise False\n",
        "        layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "      weight_updates = self.momentum  * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "      layer.weight_momentums = weight_updates\n",
        "\n",
        "      bias_updates =self.momentum  * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "      layer.bias_momentums = bias_updates\n",
        "    # Vanilla SGD updates (as before momentum update)\n",
        "    else:\n",
        "      weight_updates = -self.current_learning_rate * layer.dweights\n",
        "      bias_updates = -self.current_learning_rate * layer.dbiases\n",
        "\n",
        "    # Update weights and biases using either\n",
        "    # vanilla or momentum updates\n",
        "\n",
        "    layer.weights += weight_updates\n",
        "    layer.biases +=bias_updates\n",
        "\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations +=1\n",
        "\n",
        "# -------|------------\n",
        "#Adagrad optimizer\n",
        "# cache += parm_gradient ** 2\n",
        "# parm_updates = learning_rate  * parm_gradient /(sqrt(cache) * eps)\n",
        "\n",
        "class Optimizer_Adagrad:\n",
        "  def __init__(self,learning_rate=1, decay=0,epsilon= 1e-7):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations =0\n",
        "    self.epsilon=epsilon\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1 / (1+ self.decay * self.iterations))\n",
        "\n",
        "\n",
        "  #update parameters\n",
        "  def update_params(self,layer):\n",
        "    # If layer does not contain cache arrays, create them\n",
        "    # filled with zeros\n",
        "      if not hasattr(layer,'weight_cache'):\n",
        "        layer.weight_cache= np.zeros_like(layer.weights)\n",
        "        layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "      # Update cache with squared current gradients\n",
        "      layer.weight_cache += layer.dweights **2\n",
        "      layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "\n",
        "      # Vanilla SGD parameter update + normalization\n",
        "      # with square rooted cache\n",
        "      layer.weights += -self.current_learning_rate * layer.dweights/ (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "\n",
        "      layer.biases += -self.current_learning_rate * layer.dbiases/ (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations +=1\n",
        "\n",
        "#----------|------\n",
        "# RMSProp  Root Mean Square Propagation\n",
        "# cache = rho * cache + (1 - rho) * gradient ** 2\n",
        "# Rho is the cache memory decay rate\n",
        "\n",
        "class Optimizer_RMSprop:\n",
        "  def __init__(self,learning_rate=0.001, decay=0,epsilon= 1e-7,rho=0.9):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations =0\n",
        "    self.epsilon=epsilon\n",
        "    self.rho = rho\n",
        "\n",
        "\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1 / (1+ self.decay * self.iterations))\n",
        "\n",
        "\n",
        "  #update parameters\n",
        "  def update_params(self,layer):\n",
        "    # If layer does not contain cache arrays, create them\n",
        "    # filled with zeros\n",
        "      if not hasattr(layer,'weight_cache'):\n",
        "        layer.weight_cache= np.zeros_like(layer.weights)\n",
        "        layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "      # Update cache with squared current gradients\n",
        "      layer.weight_cache = self.rho * layer.weight_cache + (1- self.rho) * layer.dweights **2\n",
        "      layer.bias_cache =self.rho * layer.bias_cache + (1- self.rho) * layer.dbiases **2\n",
        "\n",
        "\n",
        "      # Vanilla SGD parameter update + normalization\n",
        "      # with square rooted cache\n",
        "      layer.weights += -self.current_learning_rate * layer.dweights/ (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "\n",
        "      layer.biases += -self.current_learning_rate * layer.dbiases/ (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations +=1\n",
        "\n",
        "# -----|------\n",
        "#Adam Optimizer Adaptive Momentum\n",
        "\n",
        "\n",
        "class Optimizer_Adam:\n",
        "  def __init__(self,learning_rate=0.001, decay=0,epsilon= 1e-7,beta_1=0.9,beta_2 =0.999):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations =0\n",
        "    self.epsilon=epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "\n",
        "\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1 / (1+ self.decay * self.iterations))\n",
        "\n",
        "\n",
        "  #update parameters\n",
        "  def update_params(self,layer):\n",
        "    # If layer does not contain cache arrays, create them\n",
        "    # filled with zeros\n",
        "      if not hasattr(layer,'weight_cache'):\n",
        "        layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "        layer.weight_cache= np.zeros_like(layer.weights)\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "        layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "      # Update momentum with current gradients\n",
        "      layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1-self.beta_1) * layer.dweights\n",
        "      layer.bias_momentums =self.beta_1 * layer.bias_momentums + (1-self.beta_1) * layer.dbiases\n",
        "\n",
        "      # Get corrected momentum\n",
        "      # self.iteration is 0 at first pass\n",
        "      # and we need to start with 1 here\n",
        "      weight_momentums_correctd = layer.weight_momentums/ (1-self.beta_1 ** (self.iterations + 1))\n",
        "      bias_momentums_correctd = layer.bias_momentums/ (1-self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "      # Update cache with squared current gradients\n",
        "      layer.weight_cache = self.beta_2 * layer.weight_cache  + (1-self.beta_2) * layer.dweights**2\n",
        "      layer.bias_cache = self.beta_2 * layer.bias_cache  + (1-self.beta_2) * layer.dbiases**2\n",
        "\n",
        "      # Get corrected cache\n",
        "      weight_cache_corrected = layer.weight_cache/ (1-self.beta_2 ** (self.iterations + 1))\n",
        "      bias_cache_corrected =layer.bias_cache/ (1-self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "      # Vanilla SGD parameter update + normalization\n",
        "      # with square rooted cache\n",
        "\n",
        "\n",
        "      layer.weights += -self.current_learning_rate * weight_momentums_correctd \\\n",
        "       /(np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
        "      layer.biases += -self.current_learning_rate * bias_momentums_correctd \\\n",
        "       /(np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
        "\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations +=1\n",
        "\n",
        "# -----------|-----------\n",
        "\n",
        "\n",
        "# Loss\n",
        "class Loss:\n",
        "  # Regularization loss calculation\n",
        "  def regularization_loss(self,layer):\n",
        "    # 0 by default\n",
        "    regularization_loss = 0\n",
        "    # L1 regularization - weights\n",
        "    # calculate only when factor greater than 0\n",
        "\n",
        "    if layer.weight_regularizer_l1>0:\n",
        "      regularization_loss +=layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
        "\n",
        "    # L2 regularization - weights\n",
        "    if layer.weight_regularizer_l2>0:\n",
        "      regularization_loss +=layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
        "\n",
        "    # L1 regularization -biases\n",
        "    # calculate only when factor greater than 0\n",
        "\n",
        "    if layer.bias_regularizer_l1>0:\n",
        "      regularization_loss +=layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
        "\n",
        "    # L2 regularization - biases\n",
        "    if layer.bias_regularizer_l2>0:\n",
        "      regularization_loss +=layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
        "\n",
        "    return regularization_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calculate(self, output, y):\n",
        "\n",
        "    sample_losses = self.forward(output, y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "\n",
        "    return data_loss\n",
        "\n",
        "# Cross Entropy Loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    # clip = It ensures no probabilities are exactly 0 or 1\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    #1d\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[\n",
        "      range(samples),\n",
        "      y_true\n",
        "      ]\n",
        "\n",
        "    #2d One hot encoded\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = np.sum(\n",
        "      y_pred_clipped * y_true,\n",
        "      axis=1\n",
        "      )\n",
        "\n",
        "    # no need to sum others because others are zero\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  def backward(self,dvalues,y_true):\n",
        "    samples = len(dvalues)\n",
        "    labels = len(dvalues[0])\n",
        "\n",
        "    #one hot encoded\n",
        "    if len(y_true.shape) == 1:\n",
        "      #np.eye = diagnoal 1 and other 0's\n",
        "      y_true = np.eye(labels)[y_true] #hot at y_true\n",
        "\n",
        "    #calculate gradient\n",
        "    self.dinputs = -y_true/dvalues\n",
        "    #normalize gradient\n",
        "    self.dinputs = self.dinputs/samples\n",
        "\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.activation = Activation_Softmax()\n",
        "    self.loss= Loss_CategoricalCrossentropy()\n",
        "\n",
        "  #forward pass\n",
        "  def forward(self,inputs, y_true):\n",
        "    self.activation.forward(inputs)\n",
        "    self.output = self.activation.output\n",
        "    return self.loss.calculate(self.output,y_true)\n",
        "\n",
        "  def backward(self,dvalues,y_true):\n",
        "    samples = len(dvalues)\n",
        "\n",
        "    # If labels are one-hot encoded,\n",
        "    # turn them into discrete values\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true,axix=1)\n",
        "\n",
        "    self.dinputs = dvalues.copy()\n",
        "    #calculate gradients\n",
        "    # Subtract 1 at the true class indices\n",
        "    self.dinputs[range(samples),y_true] -=1 #softmax_output - one_hot_encoded_labels\n",
        "    #normalize gradients\n",
        "    self.dinputs = self.dinputs/samples\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "  # Forward pass\n",
        "  def forward(self,y_pred,y_true):\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
        "\n",
        "    # Calculate sample-wise loss\n",
        "    sample_losses = -(y_true * np.log(y_pred_clipped) + (1-y_true) * np.log(1-y_pred_clipped))\n",
        "    sample_losses = np.mean(sample_losses , axis=-1)\n",
        "\n",
        "    return sample_losses\n",
        "\n",
        "  #Backward Pass\n",
        "  def backward(self,dvalues,y_true):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # Number of outputs in every sample\n",
        "    # We'll use the first sample to count them\n",
        "    outputs = len(dvalues[0])\n",
        "\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Calculate gradient\n",
        "    self.dinputs = -(y_true/clipped_dvalues - (1-y_true)/(1-clipped_dvalues)) / outputs\n",
        "\n",
        "    #Normalize gradent\n",
        "    self.dinputs = self.dinputs/samples\n",
        "\n",
        "# Mean Squared Error loss\n",
        "class Loss_MeanSquaredError(Loss):\n",
        "  #forward pass\n",
        "  def forward(self,y_pred,y_true):\n",
        "\n",
        "    sample_losses = np.mean((y_true-y_pred)**2, axis=-1)\n",
        "    return sample_losses\n",
        "\n",
        "  #backward pass\n",
        "  def backward(self,dvalues,y_true):\n",
        "    samples = len(dvalues)\n",
        "    outputs = len(dvalues[0])\n",
        "\n",
        "    self.dinputs = -2 * (y_true - dvalues)/outputs\n",
        "    self.dinputs  = self.dinputs /samples\n",
        "\n",
        "# Mean Absolute Error loss\n",
        "class Loss_MeanAbsoluteError(Loss):\n",
        "  def forward(self,y_pred,y_true):\n",
        "    sample_losses = np.mean(np.abs(y_true-y_pred),axis=-1 )\n",
        "    return sample_losses\n",
        "\n",
        "  def backward(self,dvalues,y_true):\n",
        "    samples = len(dvalues)\n",
        "    outputs = len(dvalues[0])\n",
        "    self.dinputs = np.sign(y_true-dvalues)/outputs\n",
        "    self.dinputs = self.dinputs /samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4a8VMsy_tTt",
        "outputId": "ef3ab84e-f849-47ea-be9a-fbf8138b6cbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nnfs) (2.0.2)\n",
            "Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()"
      ],
      "metadata": {
        "id": "Sw6fliF5_otk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "X, y = spiral_data(samples=1000, classes=3)\n",
        "\n",
        "#Architecture\n",
        "dense1 = Layer_Dense(2, 512,\n",
        "                     weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create dropout layer\n",
        "dropout1 = Layer_Dropout(0.1)\n",
        "dense2 = Layer_Dense(512, 3)\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# optimizer = Optimizer_SGD(decay=1e-3,momentum=0.9)\n",
        "# optimizer = Optimizer_Adagrad(decay=1e-4)\n",
        "# optimizer = Optimizer_RMSprop(learning_rate=0.02,decay=1e-5,rho=0.999)\n",
        "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n",
        "\n",
        "for epoch in range(10001):\n",
        "  #Forward pass\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  # Perform a forward pass through Dropout layer\n",
        "  dropout1.forward(activation1.output)\n",
        "  dense2.forward(dropout1.output)\n",
        "  data_loss = loss_activation.forward(dense2.output,y)\n",
        "\n",
        "  # Calculate regularization penalty\n",
        "  regularization_loss = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "  loss_activation.loss.regularization_loss(dense2)\n",
        "\n",
        "  # Calculate overall loss\n",
        "  loss = data_loss + regularization_loss\n",
        "\n",
        "  #accuracy\n",
        "  predictions = np.argmax(loss_activation.output,axis=1)\n",
        "  if len(y.shape) == 2:\n",
        "    y= np.argmax(y,axis=1)\n",
        "  accuracy = np.mean(predictions==y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch} | ' +\n",
        "          f' acc: {accuracy: .3f} | ' +\n",
        "          f'loss: {loss:.3f} (' +\n",
        "          f'data_loss: {data_loss:.3f}, ' +\n",
        "          f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "          f\" lr :{optimizer.current_learning_rate} \")\n",
        "\n",
        "\n",
        "  #backward pass\n",
        "  loss_activation.backward(loss_activation.output,y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  dropout1.backward(dense2.dinputs)\n",
        "  activation1.backward(dropout1.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "\n",
        "  # update weight  and biases\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()\n"
      ],
      "metadata": {
        "id": "EWmh5_nyQFKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9df004-188a-4ce6-b0c1-c89e55d58376"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 |  acc:  0.373 | loss: 1.100 (data_loss: 1.095, reg_loss: 0.005),  lr :0.05 \n",
            "epoch: 100 |  acc:  0.712 | loss: 0.730 (data_loss: 0.658, reg_loss: 0.071),  lr :0.04999752512250644 \n",
            "epoch: 200 |  acc:  0.789 | loss: 0.605 (data_loss: 0.520, reg_loss: 0.084),  lr :0.04999502549496326 \n",
            "epoch: 300 |  acc:  0.817 | loss: 0.550 (data_loss: 0.464, reg_loss: 0.085),  lr :0.049992526117345455 \n",
            "epoch: 400 |  acc:  0.841 | loss: 0.513 (data_loss: 0.430, reg_loss: 0.083),  lr :0.04999002698961558 \n",
            "epoch: 500 |  acc:  0.836 | loss: 0.516 (data_loss: 0.430, reg_loss: 0.085),  lr :0.049987528111736124 \n",
            "epoch: 600 |  acc:  0.834 | loss: 0.519 (data_loss: 0.441, reg_loss: 0.078),  lr :0.049985029483669646 \n",
            "epoch: 700 |  acc:  0.845 | loss: 0.498 (data_loss: 0.422, reg_loss: 0.076),  lr :0.049982531105378675 \n",
            "epoch: 800 |  acc:  0.852 | loss: 0.485 (data_loss: 0.405, reg_loss: 0.080),  lr :0.04998003297682575 \n",
            "epoch: 900 |  acc:  0.853 | loss: 0.466 (data_loss: 0.392, reg_loss: 0.074),  lr :0.049977535097973466 \n",
            "epoch: 1000 |  acc:  0.827 | loss: 0.526 (data_loss: 0.454, reg_loss: 0.072),  lr :0.049975037468784345 \n",
            "epoch: 1100 |  acc:  0.843 | loss: 0.499 (data_loss: 0.427, reg_loss: 0.071),  lr :0.049972540089220974 \n",
            "epoch: 1200 |  acc:  0.833 | loss: 0.513 (data_loss: 0.443, reg_loss: 0.070),  lr :0.04997004295924593 \n",
            "epoch: 1300 |  acc:  0.850 | loss: 0.460 (data_loss: 0.388, reg_loss: 0.072),  lr :0.04996754607882181 \n",
            "epoch: 1400 |  acc:  0.861 | loss: 0.457 (data_loss: 0.389, reg_loss: 0.068),  lr :0.049965049447911185 \n",
            "epoch: 1500 |  acc:  0.859 | loss: 0.467 (data_loss: 0.397, reg_loss: 0.069),  lr :0.04996255306647668 \n",
            "epoch: 1600 |  acc:  0.836 | loss: 0.488 (data_loss: 0.419, reg_loss: 0.070),  lr :0.049960056934480884 \n",
            "epoch: 1700 |  acc:  0.868 | loss: 0.469 (data_loss: 0.402, reg_loss: 0.067),  lr :0.04995756105188642 \n",
            "epoch: 1800 |  acc:  0.852 | loss: 0.482 (data_loss: 0.417, reg_loss: 0.066),  lr :0.049955065418655915 \n",
            "epoch: 1900 |  acc:  0.850 | loss: 0.451 (data_loss: 0.387, reg_loss: 0.064),  lr :0.04995257003475201 \n",
            "epoch: 2000 |  acc:  0.846 | loss: 0.453 (data_loss: 0.388, reg_loss: 0.065),  lr :0.04995007490013731 \n",
            "epoch: 2100 |  acc:  0.859 | loss: 0.464 (data_loss: 0.398, reg_loss: 0.066),  lr :0.0499475800147745 \n",
            "epoch: 2200 |  acc:  0.834 | loss: 0.494 (data_loss: 0.426, reg_loss: 0.069),  lr :0.0499450853786262 \n",
            "epoch: 2300 |  acc:  0.846 | loss: 0.452 (data_loss: 0.385, reg_loss: 0.068),  lr :0.0499425909916551 \n",
            "epoch: 2400 |  acc:  0.854 | loss: 0.458 (data_loss: 0.394, reg_loss: 0.064),  lr :0.04994009685382384 \n",
            "epoch: 2500 |  acc:  0.851 | loss: 0.438 (data_loss: 0.371, reg_loss: 0.066),  lr :0.04993760296509512 \n",
            "epoch: 2600 |  acc:  0.852 | loss: 0.482 (data_loss: 0.392, reg_loss: 0.091),  lr :0.049935109325431604 \n",
            "epoch: 2700 |  acc:  0.857 | loss: 0.451 (data_loss: 0.373, reg_loss: 0.079),  lr :0.049932615934796004 \n",
            "epoch: 2800 |  acc:  0.864 | loss: 0.438 (data_loss: 0.368, reg_loss: 0.070),  lr :0.04993012279315098 \n",
            "epoch: 2900 |  acc:  0.868 | loss: 0.455 (data_loss: 0.390, reg_loss: 0.065),  lr :0.049927629900459285 \n",
            "epoch: 3000 |  acc:  0.872 | loss: 0.439 (data_loss: 0.376, reg_loss: 0.063),  lr :0.049925137256683606 \n",
            "epoch: 3100 |  acc:  0.857 | loss: 0.434 (data_loss: 0.371, reg_loss: 0.063),  lr :0.04992264486178666 \n",
            "epoch: 3200 |  acc:  0.851 | loss: 0.464 (data_loss: 0.394, reg_loss: 0.070),  lr :0.04992015271573119 \n",
            "epoch: 3300 |  acc:  0.861 | loss: 0.450 (data_loss: 0.385, reg_loss: 0.065),  lr :0.04991766081847992 \n",
            "epoch: 3400 |  acc:  0.871 | loss: 0.436 (data_loss: 0.375, reg_loss: 0.061),  lr :0.049915169169995596 \n",
            "epoch: 3500 |  acc:  0.860 | loss: 0.435 (data_loss: 0.376, reg_loss: 0.058),  lr :0.049912677770240964 \n",
            "epoch: 3600 |  acc:  0.848 | loss: 0.472 (data_loss: 0.414, reg_loss: 0.059),  lr :0.049910186619178794 \n",
            "epoch: 3700 |  acc:  0.868 | loss: 0.459 (data_loss: 0.401, reg_loss: 0.058),  lr :0.04990769571677183 \n",
            "epoch: 3800 |  acc:  0.878 | loss: 0.425 (data_loss: 0.365, reg_loss: 0.059),  lr :0.04990520506298287 \n",
            "epoch: 3900 |  acc:  0.796 | loss: 0.643 (data_loss: 0.585, reg_loss: 0.058),  lr :0.04990271465777467 \n",
            "epoch: 4000 |  acc:  0.817 | loss: 0.540 (data_loss: 0.436, reg_loss: 0.104),  lr :0.049900224501110035 \n",
            "epoch: 4100 |  acc:  0.815 | loss: 0.521 (data_loss: 0.430, reg_loss: 0.091),  lr :0.04989773459295174 \n",
            "epoch: 4200 |  acc:  0.837 | loss: 0.502 (data_loss: 0.418, reg_loss: 0.084),  lr :0.04989524493326262 \n",
            "epoch: 4300 |  acc:  0.836 | loss: 0.503 (data_loss: 0.426, reg_loss: 0.077),  lr :0.04989275552200545 \n",
            "epoch: 4400 |  acc:  0.844 | loss: 0.497 (data_loss: 0.425, reg_loss: 0.072),  lr :0.04989026635914307 \n",
            "epoch: 4500 |  acc:  0.853 | loss: 0.471 (data_loss: 0.403, reg_loss: 0.068),  lr :0.04988777744463829 \n",
            "epoch: 4600 |  acc:  0.846 | loss: 0.469 (data_loss: 0.403, reg_loss: 0.066),  lr :0.049885288778453954 \n",
            "epoch: 4700 |  acc:  0.815 | loss: 0.515 (data_loss: 0.452, reg_loss: 0.062),  lr :0.049882800360552884 \n",
            "epoch: 4800 |  acc:  0.837 | loss: 0.494 (data_loss: 0.433, reg_loss: 0.061),  lr :0.04988031219089794 \n",
            "epoch: 4900 |  acc:  0.848 | loss: 0.472 (data_loss: 0.413, reg_loss: 0.059),  lr :0.049877824269451976 \n",
            "epoch: 5000 |  acc:  0.855 | loss: 0.445 (data_loss: 0.388, reg_loss: 0.058),  lr :0.04987533659617785 \n",
            "epoch: 5100 |  acc:  0.842 | loss: 0.468 (data_loss: 0.412, reg_loss: 0.056),  lr :0.04987284917103844 \n",
            "epoch: 5200 |  acc:  0.838 | loss: 0.492 (data_loss: 0.435, reg_loss: 0.057),  lr :0.04987036199399661 \n",
            "epoch: 5300 |  acc:  0.844 | loss: 0.477 (data_loss: 0.421, reg_loss: 0.056),  lr :0.04986787506501525 \n",
            "epoch: 5400 |  acc:  0.855 | loss: 0.468 (data_loss: 0.413, reg_loss: 0.055),  lr :0.04986538838405724 \n",
            "epoch: 5500 |  acc:  0.849 | loss: 0.466 (data_loss: 0.411, reg_loss: 0.055),  lr :0.049862901951085496 \n",
            "epoch: 5600 |  acc:  0.843 | loss: 0.475 (data_loss: 0.420, reg_loss: 0.056),  lr :0.049860415766062906 \n",
            "epoch: 5700 |  acc:  0.803 | loss: 0.599 (data_loss: 0.511, reg_loss: 0.088),  lr :0.0498579298289524 \n",
            "epoch: 5800 |  acc:  0.834 | loss: 0.503 (data_loss: 0.427, reg_loss: 0.075),  lr :0.04985544413971689 \n",
            "epoch: 5900 |  acc:  0.852 | loss: 0.469 (data_loss: 0.400, reg_loss: 0.069),  lr :0.049852958698319315 \n",
            "epoch: 6000 |  acc:  0.845 | loss: 0.485 (data_loss: 0.420, reg_loss: 0.065),  lr :0.04985047350472258 \n",
            "epoch: 6100 |  acc:  0.836 | loss: 0.499 (data_loss: 0.437, reg_loss: 0.061),  lr :0.04984798855888967 \n",
            "epoch: 6200 |  acc:  0.858 | loss: 0.475 (data_loss: 0.416, reg_loss: 0.059),  lr :0.049845503860783506 \n",
            "epoch: 6300 |  acc:  0.840 | loss: 0.463 (data_loss: 0.406, reg_loss: 0.057),  lr :0.049843019410367055 \n",
            "epoch: 6400 |  acc:  0.855 | loss: 0.477 (data_loss: 0.422, reg_loss: 0.055),  lr :0.04984053520760327 \n",
            "epoch: 6500 |  acc:  0.713 | loss: 0.702 (data_loss: 0.608, reg_loss: 0.094),  lr :0.049838051252455155 \n",
            "epoch: 6600 |  acc:  0.800 | loss: 0.559 (data_loss: 0.474, reg_loss: 0.085),  lr :0.049835567544885655 \n",
            "epoch: 6700 |  acc:  0.823 | loss: 0.521 (data_loss: 0.446, reg_loss: 0.075),  lr :0.04983308408485778 \n",
            "epoch: 6800 |  acc:  0.820 | loss: 0.532 (data_loss: 0.461, reg_loss: 0.072),  lr :0.0498306008723345 \n",
            "epoch: 6900 |  acc:  0.826 | loss: 0.509 (data_loss: 0.443, reg_loss: 0.067),  lr :0.04982811790727884 \n",
            "epoch: 7000 |  acc:  0.829 | loss: 0.509 (data_loss: 0.445, reg_loss: 0.064),  lr :0.04982563518965381 \n",
            "epoch: 7100 |  acc:  0.827 | loss: 0.499 (data_loss: 0.437, reg_loss: 0.061),  lr :0.049823152719422406 \n",
            "epoch: 7200 |  acc:  0.836 | loss: 0.502 (data_loss: 0.442, reg_loss: 0.060),  lr :0.049820670496547675 \n",
            "epoch: 7300 |  acc:  0.838 | loss: 0.499 (data_loss: 0.441, reg_loss: 0.058),  lr :0.04981818852099264 \n",
            "epoch: 7400 |  acc:  0.827 | loss: 0.515 (data_loss: 0.457, reg_loss: 0.058),  lr :0.049815706792720335 \n",
            "epoch: 7500 |  acc:  0.822 | loss: 0.524 (data_loss: 0.468, reg_loss: 0.056),  lr :0.0498132253116938 \n",
            "epoch: 7600 |  acc:  0.832 | loss: 0.479 (data_loss: 0.424, reg_loss: 0.055),  lr :0.04981074407787611 \n",
            "epoch: 7700 |  acc:  0.833 | loss: 0.467 (data_loss: 0.413, reg_loss: 0.054),  lr :0.049808263091230306 \n",
            "epoch: 7800 |  acc:  0.839 | loss: 0.489 (data_loss: 0.436, reg_loss: 0.052),  lr :0.04980578235171948 \n",
            "epoch: 7900 |  acc:  0.832 | loss: 0.493 (data_loss: 0.441, reg_loss: 0.053),  lr :0.04980330185930667 \n",
            "epoch: 8000 |  acc:  0.824 | loss: 0.484 (data_loss: 0.432, reg_loss: 0.052),  lr :0.04980082161395499 \n",
            "epoch: 8100 |  acc:  0.835 | loss: 0.472 (data_loss: 0.419, reg_loss: 0.053),  lr :0.04979834161562752 \n",
            "epoch: 8200 |  acc:  0.838 | loss: 0.471 (data_loss: 0.418, reg_loss: 0.053),  lr :0.04979586186428736 \n",
            "epoch: 8300 |  acc:  0.839 | loss: 0.488 (data_loss: 0.436, reg_loss: 0.052),  lr :0.04979338235989761 \n",
            "epoch: 8400 |  acc:  0.836 | loss: 0.490 (data_loss: 0.437, reg_loss: 0.053),  lr :0.04979090310242139 \n",
            "epoch: 8500 |  acc:  0.838 | loss: 0.484 (data_loss: 0.430, reg_loss: 0.053),  lr :0.049788424091821805 \n",
            "epoch: 8600 |  acc:  0.837 | loss: 0.485 (data_loss: 0.432, reg_loss: 0.052),  lr :0.049785945328062006 \n",
            "epoch: 8700 |  acc:  0.855 | loss: 0.462 (data_loss: 0.410, reg_loss: 0.052),  lr :0.0497834668111051 \n",
            "epoch: 8800 |  acc:  0.848 | loss: 0.467 (data_loss: 0.415, reg_loss: 0.052),  lr :0.049780988540914256 \n",
            "epoch: 8900 |  acc:  0.846 | loss: 0.479 (data_loss: 0.427, reg_loss: 0.052),  lr :0.0497785105174526 \n",
            "epoch: 9000 |  acc:  0.838 | loss: 0.474 (data_loss: 0.421, reg_loss: 0.053),  lr :0.04977603274068329 \n",
            "epoch: 9100 |  acc:  0.840 | loss: 0.504 (data_loss: 0.451, reg_loss: 0.053),  lr :0.04977355521056952 \n",
            "epoch: 9200 |  acc:  0.767 | loss: 0.648 (data_loss: 0.549, reg_loss: 0.099),  lr :0.049771077927074414 \n",
            "epoch: 9300 |  acc:  0.793 | loss: 0.599 (data_loss: 0.510, reg_loss: 0.089),  lr :0.0497686008901612 \n",
            "epoch: 9400 |  acc:  0.800 | loss: 0.592 (data_loss: 0.510, reg_loss: 0.082),  lr :0.04976612409979302 \n",
            "epoch: 9500 |  acc:  0.804 | loss: 0.565 (data_loss: 0.488, reg_loss: 0.077),  lr :0.0497636475559331 \n",
            "epoch: 9600 |  acc:  0.804 | loss: 0.572 (data_loss: 0.489, reg_loss: 0.082),  lr :0.049761171258544616 \n",
            "epoch: 9700 |  acc:  0.817 | loss: 0.551 (data_loss: 0.479, reg_loss: 0.072),  lr :0.0497586952075908 \n",
            "epoch: 9800 |  acc:  0.757 | loss: 0.662 (data_loss: 0.563, reg_loss: 0.099),  lr :0.04975621940303483 \n",
            "epoch: 9900 |  acc:  0.779 | loss: 0.603 (data_loss: 0.517, reg_loss: 0.086),  lr :0.049753743844839965 \n",
            "epoch: 10000 |  acc:  0.796 | loss: 0.565 (data_loss: 0.487, reg_loss: 0.078),  lr :0.04975126853296942 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the model\n",
        "# Create test dataset\n",
        "X_test, y_test = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Perform a forward pass of our testing data through this layer\n",
        "dense1.forward(X_test)\n",
        "# Perform a forward pass through activation function\n",
        "# takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "# Perform a forward pass through second Dense layer\n",
        "# takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "# Perform a forward pass through the activation/loss function\n",
        "# takes the output of second dense layer here and returns loss\n",
        "loss = loss_activation.forward(dense2.output, y_test)\n",
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along first axis\n",
        "predictions = np.argmax(loss_activation.output, axis=1)\n",
        "if len(y_test.shape) == 2:\n",
        "  y_test = np.argmax(y_test, axis=1)\n",
        "  accuracy = np.mean(predictions==y_test)\n",
        "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
      ],
      "metadata": {
        "id": "uDF-fuJoRao5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c342834e-7117-4f5d-a317-5ad46f7aede2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation, acc: 0.796, loss: 0.468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for binary regression\n",
        "#Data\n",
        "X, y = spiral_data(samples=100, classes=2)\n",
        "# Reshape labels to be a list of lists\n",
        "# Inner list contains one output (either 0 or 1)\n",
        "# per each output neuron, 1 in this case\n",
        "y = y.reshape(-1,1)\n",
        "\n",
        "#Architecture\n",
        "dense1 = Layer_Dense(2, 64,\n",
        "                     weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "activation1 = Activation_ReLU()\n",
        "# Create dropout layer\n",
        "# dropout1 = Layer_Dropout(0.1)\n",
        "dense2 = Layer_Dense(64, 1)\n",
        "activation2 = Activation_Sigmoid()\n",
        "loss_function = Loss_BinaryCrossentropy()\n",
        "\n",
        "# optimizer = Optimizer_SGD(decay=1e-3,momentum=0.9)\n",
        "# optimizer = Optimizer_Adagrad(decay=1e-4)\n",
        "# optimizer = Optimizer_RMSprop(learning_rate=0.02,decay=1e-5,rho=0.999)\n",
        "optimizer = Optimizer_Adam( decay=5e-7)\n",
        "\n",
        "for epoch in range(10001):\n",
        "  #Forward pass Input X → dense1 → ReLU → dense2 → Softmax → Loss.\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  activation2.forward(dense2.output)\n",
        "  data_loss = loss_function.calculate(activation2.output,y)\n",
        "\n",
        "  # Calculate regularization penalty\n",
        "  regularization_loss = loss_function.regularization_loss(dense1) + \\\n",
        "  loss_function.regularization_loss(dense2)\n",
        "\n",
        "  # Calculate overall loss\n",
        "  loss = data_loss + regularization_loss\n",
        "\n",
        "  #accuracy\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # Part in the brackets returns a binary mask - array consisting\n",
        "  # of True/False values, multiplying it by 1 changes it into array\n",
        "  # of 1s and 0s\n",
        "  predictions = (activation2.output >0.5) * 1\n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch} | ' +\n",
        "          f' acc: {accuracy: .3f} | ' +\n",
        "          f'loss: {loss:.3f} (' +\n",
        "          f'data_loss: {data_loss:.3f}, ' +\n",
        "          f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "          f\" lr :{optimizer.current_learning_rate} \")\n",
        "\n",
        "\n",
        "  #backward pass Loss\n",
        "  loss_function.backward(activation2.output,y)\n",
        "  activation2.backward(loss_function.dinputs)\n",
        "  dense2.backward(activation2.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "\n",
        "\n",
        "  # update weight  and biases\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()"
      ],
      "metadata": {
        "id": "3Orqu9zNRnFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9d10fb-9527-4850-a587-d349ecce3e20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 |  acc:  0.570 | loss: 0.691 (data_loss: 0.691, reg_loss: 0.001),  lr :0.001 \n",
            "epoch: 100 |  acc:  0.605 | loss: 0.664 (data_loss: 0.662, reg_loss: 0.001),  lr :0.0009999505024501287 \n",
            "epoch: 200 |  acc:  0.605 | loss: 0.656 (data_loss: 0.654, reg_loss: 0.002),  lr :0.0009999005098992651 \n",
            "epoch: 300 |  acc:  0.610 | loss: 0.651 (data_loss: 0.648, reg_loss: 0.002),  lr :0.000999850522346909 \n",
            "epoch: 400 |  acc:  0.615 | loss: 0.643 (data_loss: 0.640, reg_loss: 0.003),  lr :0.0009998005397923115 \n",
            "epoch: 500 |  acc:  0.630 | loss: 0.634 (data_loss: 0.630, reg_loss: 0.005),  lr :0.0009997505622347225 \n",
            "epoch: 600 |  acc:  0.645 | loss: 0.624 (data_loss: 0.618, reg_loss: 0.006),  lr :0.0009997005896733929 \n",
            "epoch: 700 |  acc:  0.675 | loss: 0.612 (data_loss: 0.604, reg_loss: 0.008),  lr :0.0009996506221075735 \n",
            "epoch: 800 |  acc:  0.695 | loss: 0.600 (data_loss: 0.590, reg_loss: 0.010),  lr :0.000999600659536515 \n",
            "epoch: 900 |  acc:  0.725 | loss: 0.588 (data_loss: 0.576, reg_loss: 0.012),  lr :0.0009995507019594694 \n",
            "epoch: 1000 |  acc:  0.740 | loss: 0.575 (data_loss: 0.561, reg_loss: 0.015),  lr :0.000999500749375687 \n",
            "epoch: 1100 |  acc:  0.745 | loss: 0.565 (data_loss: 0.548, reg_loss: 0.017),  lr :0.0009994508017844195 \n",
            "epoch: 1200 |  acc:  0.740 | loss: 0.555 (data_loss: 0.536, reg_loss: 0.019),  lr :0.0009994008591849186 \n",
            "epoch: 1300 |  acc:  0.750 | loss: 0.545 (data_loss: 0.524, reg_loss: 0.021),  lr :0.0009993509215764362 \n",
            "epoch: 1400 |  acc:  0.770 | loss: 0.536 (data_loss: 0.512, reg_loss: 0.024),  lr :0.0009993009889582235 \n",
            "epoch: 1500 |  acc:  0.785 | loss: 0.525 (data_loss: 0.499, reg_loss: 0.027),  lr :0.0009992510613295335 \n",
            "epoch: 1600 |  acc:  0.800 | loss: 0.516 (data_loss: 0.486, reg_loss: 0.030),  lr :0.0009992011386896176 \n",
            "epoch: 1700 |  acc:  0.810 | loss: 0.507 (data_loss: 0.474, reg_loss: 0.033),  lr :0.0009991512210377285 \n",
            "epoch: 1800 |  acc:  0.810 | loss: 0.498 (data_loss: 0.463, reg_loss: 0.035),  lr :0.0009991013083731183 \n",
            "epoch: 1900 |  acc:  0.815 | loss: 0.490 (data_loss: 0.453, reg_loss: 0.037),  lr :0.0009990514006950402 \n",
            "epoch: 2000 |  acc:  0.830 | loss: 0.483 (data_loss: 0.443, reg_loss: 0.040),  lr :0.0009990014980027463 \n",
            "epoch: 2100 |  acc:  0.855 | loss: 0.474 (data_loss: 0.432, reg_loss: 0.042),  lr :0.0009989516002954898 \n",
            "epoch: 2200 |  acc:  0.860 | loss: 0.460 (data_loss: 0.416, reg_loss: 0.045),  lr :0.000998901707572524 \n",
            "epoch: 2300 |  acc:  0.885 | loss: 0.447 (data_loss: 0.400, reg_loss: 0.048),  lr :0.0009988518198331018 \n",
            "epoch: 2400 |  acc:  0.895 | loss: 0.436 (data_loss: 0.385, reg_loss: 0.051),  lr :0.0009988019370764769 \n",
            "epoch: 2500 |  acc:  0.905 | loss: 0.426 (data_loss: 0.373, reg_loss: 0.053),  lr :0.0009987520593019025 \n",
            "epoch: 2600 |  acc:  0.910 | loss: 0.417 (data_loss: 0.361, reg_loss: 0.056),  lr :0.000998702186508632 \n",
            "epoch: 2700 |  acc:  0.905 | loss: 0.405 (data_loss: 0.346, reg_loss: 0.059),  lr :0.00099865231869592 \n",
            "epoch: 2800 |  acc:  0.910 | loss: 0.394 (data_loss: 0.332, reg_loss: 0.062),  lr :0.0009986024558630198 \n",
            "epoch: 2900 |  acc:  0.910 | loss: 0.383 (data_loss: 0.319, reg_loss: 0.064),  lr :0.0009985525980091856 \n",
            "epoch: 3000 |  acc:  0.920 | loss: 0.374 (data_loss: 0.308, reg_loss: 0.067),  lr :0.000998502745133672 \n",
            "epoch: 3100 |  acc:  0.915 | loss: 0.366 (data_loss: 0.297, reg_loss: 0.069),  lr :0.0009984528972357331 \n",
            "epoch: 3200 |  acc:  0.920 | loss: 0.358 (data_loss: 0.288, reg_loss: 0.070),  lr :0.0009984030543146237 \n",
            "epoch: 3300 |  acc:  0.920 | loss: 0.351 (data_loss: 0.279, reg_loss: 0.072),  lr :0.0009983532163695982 \n",
            "epoch: 3400 |  acc:  0.930 | loss: 0.345 (data_loss: 0.271, reg_loss: 0.073),  lr :0.000998303383399912 \n",
            "epoch: 3500 |  acc:  0.940 | loss: 0.338 (data_loss: 0.264, reg_loss: 0.074),  lr :0.0009982535554048193 \n",
            "epoch: 3600 |  acc:  0.935 | loss: 0.332 (data_loss: 0.257, reg_loss: 0.075),  lr :0.000998203732383576 \n",
            "epoch: 3700 |  acc:  0.935 | loss: 0.327 (data_loss: 0.251, reg_loss: 0.076),  lr :0.0009981539143354365 \n",
            "epoch: 3800 |  acc:  0.940 | loss: 0.322 (data_loss: 0.245, reg_loss: 0.076),  lr :0.0009981041012596574 \n",
            "epoch: 3900 |  acc:  0.940 | loss: 0.316 (data_loss: 0.240, reg_loss: 0.077),  lr :0.0009980542931554933 \n",
            "epoch: 4000 |  acc:  0.940 | loss: 0.311 (data_loss: 0.234, reg_loss: 0.078),  lr :0.0009980044900222008 \n",
            "epoch: 4100 |  acc:  0.945 | loss: 0.306 (data_loss: 0.228, reg_loss: 0.078),  lr :0.0009979546918590348 \n",
            "epoch: 4200 |  acc:  0.945 | loss: 0.301 (data_loss: 0.222, reg_loss: 0.079),  lr :0.0009979048986652524 \n",
            "epoch: 4300 |  acc:  0.945 | loss: 0.296 (data_loss: 0.216, reg_loss: 0.080),  lr :0.000997855110440109 \n",
            "epoch: 4400 |  acc:  0.945 | loss: 0.292 (data_loss: 0.211, reg_loss: 0.081),  lr :0.0009978053271828614 \n",
            "epoch: 4500 |  acc:  0.950 | loss: 0.287 (data_loss: 0.207, reg_loss: 0.081),  lr :0.0009977555488927658 \n",
            "epoch: 4600 |  acc:  0.950 | loss: 0.284 (data_loss: 0.203, reg_loss: 0.081),  lr :0.000997705775569079 \n",
            "epoch: 4700 |  acc:  0.950 | loss: 0.280 (data_loss: 0.199, reg_loss: 0.081),  lr :0.0009976560072110577 \n",
            "epoch: 4800 |  acc:  0.950 | loss: 0.276 (data_loss: 0.195, reg_loss: 0.081),  lr :0.0009976062438179587 \n",
            "epoch: 4900 |  acc:  0.950 | loss: 0.273 (data_loss: 0.192, reg_loss: 0.081),  lr :0.0009975564853890394 \n",
            "epoch: 5000 |  acc:  0.950 | loss: 0.270 (data_loss: 0.189, reg_loss: 0.081),  lr :0.000997506731923557 \n",
            "epoch: 5100 |  acc:  0.950 | loss: 0.267 (data_loss: 0.186, reg_loss: 0.081),  lr :0.0009974569834207687 \n",
            "epoch: 5200 |  acc:  0.950 | loss: 0.264 (data_loss: 0.183, reg_loss: 0.080),  lr :0.0009974072398799322 \n",
            "epoch: 5300 |  acc:  0.950 | loss: 0.261 (data_loss: 0.181, reg_loss: 0.080),  lr :0.0009973575013003048 \n",
            "epoch: 5400 |  acc:  0.950 | loss: 0.258 (data_loss: 0.179, reg_loss: 0.079),  lr :0.0009973077676811448 \n",
            "epoch: 5500 |  acc:  0.950 | loss: 0.256 (data_loss: 0.177, reg_loss: 0.079),  lr :0.00099725803902171 \n",
            "epoch: 5600 |  acc:  0.950 | loss: 0.253 (data_loss: 0.175, reg_loss: 0.078),  lr :0.0009972083153212581 \n",
            "epoch: 5700 |  acc:  0.950 | loss: 0.250 (data_loss: 0.172, reg_loss: 0.078),  lr :0.000997158596579048 \n",
            "epoch: 5800 |  acc:  0.955 | loss: 0.247 (data_loss: 0.170, reg_loss: 0.077),  lr :0.0009971088827943377 \n",
            "epoch: 5900 |  acc:  0.950 | loss: 0.245 (data_loss: 0.168, reg_loss: 0.077),  lr :0.0009970591739663862 \n",
            "epoch: 6000 |  acc:  0.955 | loss: 0.243 (data_loss: 0.167, reg_loss: 0.076),  lr :0.0009970094700944517 \n",
            "epoch: 6100 |  acc:  0.950 | loss: 0.240 (data_loss: 0.165, reg_loss: 0.075),  lr :0.0009969597711777935 \n",
            "epoch: 6200 |  acc:  0.950 | loss: 0.238 (data_loss: 0.164, reg_loss: 0.075),  lr :0.00099691007721567 \n",
            "epoch: 6300 |  acc:  0.955 | loss: 0.236 (data_loss: 0.162, reg_loss: 0.074),  lr :0.000996860388207341 \n",
            "epoch: 6400 |  acc:  0.955 | loss: 0.234 (data_loss: 0.160, reg_loss: 0.074),  lr :0.0009968107041520655 \n",
            "epoch: 6500 |  acc:  0.955 | loss: 0.232 (data_loss: 0.159, reg_loss: 0.073),  lr :0.000996761025049103 \n",
            "epoch: 6600 |  acc:  0.955 | loss: 0.230 (data_loss: 0.158, reg_loss: 0.072),  lr :0.000996711350897713 \n",
            "epoch: 6700 |  acc:  0.955 | loss: 0.228 (data_loss: 0.156, reg_loss: 0.072),  lr :0.0009966616816971556 \n",
            "epoch: 6800 |  acc:  0.955 | loss: 0.226 (data_loss: 0.155, reg_loss: 0.071),  lr :0.00099661201744669 \n",
            "epoch: 6900 |  acc:  0.955 | loss: 0.224 (data_loss: 0.154, reg_loss: 0.071),  lr :0.0009965623581455767 \n",
            "epoch: 7000 |  acc:  0.955 | loss: 0.222 (data_loss: 0.152, reg_loss: 0.070),  lr :0.000996512703793076 \n",
            "epoch: 7100 |  acc:  0.955 | loss: 0.221 (data_loss: 0.151, reg_loss: 0.070),  lr :0.0009964630543884481 \n",
            "epoch: 7200 |  acc:  0.955 | loss: 0.219 (data_loss: 0.150, reg_loss: 0.069),  lr :0.0009964134099309536 \n",
            "epoch: 7300 |  acc:  0.955 | loss: 0.217 (data_loss: 0.149, reg_loss: 0.068),  lr :0.0009963637704198528 \n",
            "epoch: 7400 |  acc:  0.955 | loss: 0.216 (data_loss: 0.148, reg_loss: 0.068),  lr :0.0009963141358544066 \n",
            "epoch: 7500 |  acc:  0.955 | loss: 0.214 (data_loss: 0.147, reg_loss: 0.067),  lr :0.000996264506233876 \n",
            "epoch: 7600 |  acc:  0.955 | loss: 0.212 (data_loss: 0.146, reg_loss: 0.067),  lr :0.0009962148815575223 \n",
            "epoch: 7700 |  acc:  0.955 | loss: 0.211 (data_loss: 0.145, reg_loss: 0.066),  lr :0.000996165261824606 \n",
            "epoch: 7800 |  acc:  0.955 | loss: 0.209 (data_loss: 0.144, reg_loss: 0.066),  lr :0.0009961156470343895 \n",
            "epoch: 7900 |  acc:  0.955 | loss: 0.208 (data_loss: 0.143, reg_loss: 0.065),  lr :0.0009960660371861334 \n",
            "epoch: 8000 |  acc:  0.955 | loss: 0.206 (data_loss: 0.142, reg_loss: 0.064),  lr :0.0009960164322790998 \n",
            "epoch: 8100 |  acc:  0.955 | loss: 0.205 (data_loss: 0.141, reg_loss: 0.064),  lr :0.0009959668323125503 \n",
            "epoch: 8200 |  acc:  0.955 | loss: 0.204 (data_loss: 0.140, reg_loss: 0.063),  lr :0.000995917237285747 \n",
            "epoch: 8300 |  acc:  0.955 | loss: 0.202 (data_loss: 0.139, reg_loss: 0.063),  lr :0.000995867647197952 \n",
            "epoch: 8400 |  acc:  0.955 | loss: 0.201 (data_loss: 0.138, reg_loss: 0.062),  lr :0.0009958180620484277 \n",
            "epoch: 8500 |  acc:  0.955 | loss: 0.200 (data_loss: 0.138, reg_loss: 0.062),  lr :0.0009957684818364362 \n",
            "epoch: 8600 |  acc:  0.955 | loss: 0.198 (data_loss: 0.137, reg_loss: 0.062),  lr :0.0009957189065612402 \n",
            "epoch: 8700 |  acc:  0.955 | loss: 0.197 (data_loss: 0.136, reg_loss: 0.061),  lr :0.000995669336222102 \n",
            "epoch: 8800 |  acc:  0.955 | loss: 0.196 (data_loss: 0.135, reg_loss: 0.061),  lr :0.000995619770818285 \n",
            "epoch: 8900 |  acc:  0.955 | loss: 0.194 (data_loss: 0.134, reg_loss: 0.060),  lr :0.0009955702103490519 \n",
            "epoch: 9000 |  acc:  0.955 | loss: 0.193 (data_loss: 0.134, reg_loss: 0.060),  lr :0.000995520654813666 \n",
            "epoch: 9100 |  acc:  0.955 | loss: 0.192 (data_loss: 0.132, reg_loss: 0.059),  lr :0.0009954711042113903 \n",
            "epoch: 9200 |  acc:  0.960 | loss: 0.190 (data_loss: 0.131, reg_loss: 0.059),  lr :0.0009954215585414883 \n",
            "epoch: 9300 |  acc:  0.955 | loss: 0.189 (data_loss: 0.130, reg_loss: 0.058),  lr :0.000995372017803224 \n",
            "epoch: 9400 |  acc:  0.960 | loss: 0.187 (data_loss: 0.129, reg_loss: 0.058),  lr :0.0009953224819958604 \n",
            "epoch: 9500 |  acc:  0.960 | loss: 0.186 (data_loss: 0.128, reg_loss: 0.057),  lr :0.000995272951118662 \n",
            "epoch: 9600 |  acc:  0.955 | loss: 0.185 (data_loss: 0.128, reg_loss: 0.057),  lr :0.0009952234251708924 \n",
            "epoch: 9700 |  acc:  0.955 | loss: 0.184 (data_loss: 0.127, reg_loss: 0.057),  lr :0.000995173904151816 \n",
            "epoch: 9800 |  acc:  0.955 | loss: 0.183 (data_loss: 0.126, reg_loss: 0.056),  lr :0.0009951243880606966 \n",
            "epoch: 9900 |  acc:  0.955 | loss: 0.182 (data_loss: 0.126, reg_loss: 0.056),  lr :0.0009950748768967994 \n",
            "epoch: 10000 |  acc:  0.960 | loss: 0.181 (data_loss: 0.125, reg_loss: 0.055),  lr :0.0009950253706593885 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for bianary too\n",
        "# Validate the model\n",
        "# Create test dataset\n",
        "X_test, y_test = spiral_data(samples=100, classes=2)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "# Perform a forward pass of our testing data through this layer\n",
        "dense1.forward(X_test)\n",
        "# Perform a forward pass through activation function\n",
        "# takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "# Perform a forward pass through second Dense layer\n",
        "# takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "# Perform a forward pass through the activation/loss function\n",
        "# takes the output of second dense layer here and returns loss\n",
        "loss = loss_function.calculate(activation2.output, y_test)\n",
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along first axis\n",
        "predictions = (activation2.output > 0.5) * 1\n",
        "accuracy = np.mean(predictions==y_test)\n",
        "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
      ],
      "metadata": {
        "id": "JMRaBWZuSLao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1a0e73-9bb3-450a-b6b6-2fe5117e1636"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation, acc: 0.910, loss: 0.440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ydexWy3IRhSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5ed1f6-a08a-475f-e27e-a7e7659232fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 |  acc:  0.002 | loss: 0.495 (data_loss: 0.495, reg_loss: 0.000),  lr :0.005 \n",
            "epoch: 100 |  acc:  0.010 | loss: 0.067 (data_loss: 0.067, reg_loss: 0.000),  lr :0.004549590536851684 \n",
            "epoch: 200 |  acc:  0.169 | loss: 0.003 (data_loss: 0.003, reg_loss: 0.000),  lr :0.004170141784820684 \n",
            "epoch: 300 |  acc:  0.403 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.003849114703618168 \n",
            "epoch: 400 |  acc:  0.603 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0035739814152966403 \n",
            "epoch: 500 |  acc:  0.680 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.00333555703802535 \n",
            "epoch: 600 |  acc:  0.499 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0031269543464665416 \n",
            "epoch: 700 |  acc:  0.798 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.002942907592701589 \n",
            "epoch: 800 |  acc:  0.809 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0027793218454697055 \n",
            "epoch: 900 |  acc:  0.841 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0026329647182727752 \n",
            "epoch: 1000 |  acc:  0.229 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.002501250625312656 \n",
            "epoch: 1100 |  acc:  0.859 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0023820867079561697 \n",
            "epoch: 1200 |  acc:  0.870 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.002273760800363802 \n",
            "epoch: 1300 |  acc:  0.877 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.002174858634188778 \n",
            "epoch: 1400 |  acc:  0.884 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0020842017507294707 \n",
            "epoch: 1500 |  acc:  0.896 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0020008003201280513 \n",
            "epoch: 1600 |  acc:  0.901 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.001923816852635629 \n",
            "epoch: 1700 |  acc:  0.913 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.001852537977028529 \n",
            "epoch: 1800 |  acc:  0.042 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0017863522686673815 \n",
            "epoch: 1900 |  acc:  0.926 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0017247326664367024 \n",
            "epoch: 2000 |  acc:  0.935 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0016672224074691564 \n",
            "epoch: 2100 |  acc:  0.936 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0016134236850596968 \n",
            "epoch: 2200 |  acc:  0.122 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0015629884338855893 \n",
            "epoch: 2300 |  acc:  0.945 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0015156107911488332 \n",
            "epoch: 2400 |  acc:  0.949 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0014710208884966167 \n",
            "epoch: 2500 |  acc:  0.934 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0014289797084881396 \n",
            "epoch: 2600 |  acc:  0.959 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.001389274798555154 \n",
            "epoch: 2700 |  acc:  0.821 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0013517166801838335 \n",
            "epoch: 2800 |  acc:  0.964 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0013161358252171624 \n",
            "epoch: 2900 |  acc:  0.958 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0012823800974608873 \n",
            "epoch: 3000 |  acc:  0.966 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0012503125781445363 \n",
            "epoch: 3100 |  acc:  0.965 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0012198097096852891 \n",
            "epoch: 3200 |  acc:  0.038 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0011907597046915933 \n",
            "epoch: 3300 |  acc:  0.969 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0011630611770179114 \n",
            "epoch: 3400 |  acc:  0.516 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0011366219595362584 \n",
            "epoch: 3500 |  acc:  0.970 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0011113580795732384 \n",
            "epoch: 3600 |  acc:  0.943 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0010871928680147858 \n",
            "epoch: 3700 |  acc:  0.971 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0010640561821664183 \n",
            "epoch: 3800 |  acc:  0.927 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0010418837257762034 \n",
            "epoch: 3900 |  acc:  0.976 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0010206164523372118 \n",
            "epoch: 4000 |  acc:  0.848 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0010002000400080014 \n",
            "epoch: 4100 |  acc:  0.975 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0009805844283192783 \n",
            "epoch: 4200 |  acc:  0.253 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0009617234083477593 \n",
            "epoch: 4300 |  acc:  0.978 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0009435742592942063 \n",
            "epoch: 4400 |  acc:  0.131 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0009260974254491572 \n",
            "epoch: 4500 |  acc:  0.978 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0009092562284051646 \n",
            "epoch: 4600 |  acc:  0.244 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.000893016610108948 \n",
            "epoch: 4700 |  acc:  0.978 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0008773469029654326 \n",
            "epoch: 4800 |  acc:  0.979 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.000862217623728229 \n",
            "epoch: 4900 |  acc:  0.979 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0008476012883539582 \n",
            "epoch: 5000 |  acc:  0.979 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0008334722453742291 \n",
            "epoch: 5100 |  acc:  0.976 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0008198065256599442 \n",
            "epoch: 5200 |  acc:  0.980 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0008065817067268914 \n",
            "epoch: 5300 |  acc:  0.240 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0007937767899666614 \n",
            "epoch: 5400 |  acc:  0.981 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0007813720893889669 \n",
            "epoch: 5500 |  acc:  0.980 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0007693491306354824 \n",
            "epoch: 5600 |  acc:  0.984 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0007576905591756327 \n",
            "epoch: 5700 |  acc:  0.982 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0007463800567248844 \n",
            "epoch: 5800 |  acc:  0.980 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0007354022650389764 \n",
            "epoch: 5900 |  acc:  0.982 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0007247427163357008 \n",
            "epoch: 6000 |  acc:  0.923 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.000714387769681383 \n",
            "epoch: 6100 |  acc:  0.983 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0007043245527539089 \n",
            "epoch: 6200 |  acc:  0.983 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006945409084595084 \n",
            "epoch: 6300 |  acc:  0.082 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006850253459377996 \n",
            "epoch: 6400 |  acc:  0.984 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006757669955399379 \n",
            "epoch: 6500 |  acc:  0.984 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006667555674089878 \n",
            "epoch: 6600 |  acc:  0.985 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006579813133307014 \n",
            "epoch: 6700 |  acc:  0.982 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006494349915573451 \n",
            "epoch: 6800 |  acc:  0.985 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006411078343377356 \n",
            "epoch: 6900 |  acc:  0.233 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.00063299151791366 \n",
            "epoch: 7000 |  acc:  0.986 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006250781347668457 \n",
            "epoch: 7100 |  acc:  0.986 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006173601679219657 \n",
            "epoch: 7200 |  acc:  0.982 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006098304671301379 \n",
            "epoch: 7300 |  acc:  0.986 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0006024822267743102 \n",
            "epoch: 7400 |  acc:  0.950 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005953089653530181 \n",
            "epoch: 7500 |  acc:  0.986 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.000588304506412519 \n",
            "epoch: 7600 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005814629608093965 \n",
            "epoch: 7700 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005747787101965744 \n",
            "epoch: 7800 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005682463916354131 \n",
            "epoch: 7900 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005618608832453085 \n",
            "epoch: 8000 |  acc:  0.982 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.00055561729081009 \n",
            "epoch: 8100 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005495109352676119 \n",
            "epoch: 8200 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005435373410153278 \n",
            "epoch: 8300 |  acc:  0.927 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005376922249704269 \n",
            "epoch: 8400 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005319714863283328 \n",
            "epoch: 8500 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005263711969681019 \n",
            "epoch: 8600 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005208875924575476 \n",
            "epoch: 8700 |  acc:  0.682 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005155170636148056 \n",
            "epoch: 8800 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005102561485865905 \n",
            "epoch: 8900 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005051015254066068 \n",
            "epoch: 9000 |  acc:  0.986 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0005000500050005 \n",
            "epoch: 9100 |  acc:  0.986 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0004950985246063966 \n",
            "epoch: 9200 |  acc:  0.988 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0004902441415825081 \n",
            "epoch: 9300 |  acc:  0.988 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0004854840275754928 \n",
            "epoch: 9400 |  acc:  0.989 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.0004808154630252909 \n",
            "epoch: 9500 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.00047623583198399844 \n",
            "epoch: 9600 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.00047174261722804036 \n",
            "epoch: 9700 |  acc:  0.990 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.00046733339564445275 \n",
            "epoch: 9800 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.00046300583387350687 \n",
            "epoch: 9900 |  acc:  0.987 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.00045875768419121016 \n",
            "epoch: 10000 |  acc:  0.209 | loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),  lr :0.00045458678061641964 \n"
          ]
        }
      ],
      "source": [
        "# Regression Model Training\n",
        "from nnfs.datasets import sine_data\n",
        "X, y =sine_data()\n",
        "\n",
        "#Architecture\n",
        "dense1 = Layer_Dense(1, 64)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(64, 64)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "dense3 = Layer_Dense(64, 1)\n",
        "activation3 = Activation_Linear()\n",
        "\n",
        "loss_function = Loss_MeanSquaredError()\n",
        "\n",
        "\n",
        "optimizer = Optimizer_Adam(learning_rate=0.005,decay=1e-3)\n",
        "accuracy_precision = np.std(y)/250\n",
        "for epoch in range(10001):\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  activation2.forward(dense2.output)\n",
        "  dense3.forward(activation2.output)\n",
        "  activation3.forward(dense3.output)\n",
        "  data_loss = loss_function.calculate(activation3.output,y)\n",
        "\n",
        "  # Calculate regularization penalty\n",
        "  regularization_loss = \\\n",
        "    loss_function.regularization_loss(dense1) + \\\n",
        "    loss_function.regularization_loss(dense2) + \\\n",
        "    loss_function.regularization_loss(dense3)\n",
        "\n",
        "  # Calculate overall loss\n",
        "  loss = data_loss + regularization_loss\n",
        "\n",
        "  predictions = activation3.output\n",
        "  accuracy = np.mean(np.absolute(predictions-y)<accuracy_precision)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch} | ' +\n",
        "          f' acc: {accuracy: .3f} | ' +\n",
        "          f'loss: {loss:.3f} (' +\n",
        "          f'data_loss: {data_loss:.3f}, ' +\n",
        "          f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "          f\" lr :{optimizer.current_learning_rate} \")\n",
        "\n",
        "\n",
        "  #backward pass Loss\n",
        "  loss_function.backward(activation3.output,y)\n",
        "  activation3.backward(loss_function.dinputs)\n",
        "  dense3.backward(activation3.dinputs)\n",
        "  activation2.backward(dense3.dinputs)\n",
        "  dense2.backward(activation2.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "\n",
        "\n",
        "  # update weight  and biases\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.update_params(dense3)\n",
        "  optimizer.post_update_params()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "X_test, y_test = sine_data()\n",
        "dense1.forward(X_test)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "dense3.forward(activation2.output)\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "plt.plot(X_test,y_test)\n",
        "plt.plot(X_test,activation3.output)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VyUuTY5vwZSi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "e7315396-dc89-44a9-9d4e-427c0517a7db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}